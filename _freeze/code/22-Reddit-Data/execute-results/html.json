{
  "hash": "7fac44e31f08827c067600998e735a18",
  "result": {
    "markdown": "# Reddit Data Collection and Analysis with PSAW\n\nTo collect Reddit data, we're going to use the [Pushift API](https://www.reddit.com/r/pushshift/comments/bcxguf/new_to_pushshift_read_this_faq/), specifically a Python wrapper for the Pushshift API called [PSAW](https://github.com/dmarx/psaw) (PushShift API Wrapper). Why are we using the Pushshift API instead of the official Reddit API, and PSAW instead of Pushshift itself?\n\nWell, as Pushshift's creator Jason Baumgartner and his co-authors describe it in their [published paper](https://arxiv.org/pdf/2001.08435.pdf), \"Pushshift makes it\nmuch easier for researchers to query and retrieve historical Reddit data, provides extended functionality by providing fulltext search against comments and submissions, and has larger single query limits.\" PSAW, meanwhile, makes it easier to work with Pushshift and provides better documentation.\n\n## Install PSAW\n\nTo use PSAW, we first need to install it.\n\n::: {.cell ExecuteTime='{\"end_time\":\"2022-11-07T11:15:01.729529Z\",\"start_time\":\"2022-11-07T11:14:59.139780Z\"}' execution_count=1}\n``` {.python .cell-code}\n!pip install psaw\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRequirement already satisfied: psaw in /Users/paschalis/opt/anaconda3/lib/python3.8/site-packages (0.1.0)\r\nRequirement already satisfied: requests in /Users/paschalis/opt/anaconda3/lib/python3.8/site-packages (from psaw) (2.28.1)\r\nRequirement already satisfied: Click in /Users/paschalis/opt/anaconda3/lib/python3.8/site-packages (from psaw) (8.1.3)\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/paschalis/opt/anaconda3/lib/python3.8/site-packages (from requests->psaw) (1.26.11)\r\nRequirement already satisfied: certifi>=2017.4.17 in /Users/paschalis/opt/anaconda3/lib/python3.8/site-packages (from requests->psaw) (2022.9.24)\r\nRequirement already satisfied: charset-normalizer<3,>=2 in /Users/paschalis/opt/anaconda3/lib/python3.8/site-packages (from requests->psaw) (2.0.12)\r\nRequirement already satisfied: idna<4,>=2.5 in /Users/paschalis/opt/anaconda3/lib/python3.8/site-packages (from requests->psaw) (3.3)\r\n```\n:::\n:::\n\n\nThen we will import pandas for eventually working with the collected data, and we will change pandas default display setting to make our DataFrame columns wider.\n\n::: {.cell ExecuteTime='{\"end_time\":\"2022-11-07T15:39:25.278393Z\",\"start_time\":\"2022-11-07T15:39:25.274298Z\"}' execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\npd.options.display.max_colwidth = 500\npd.options.display.max_columns = 50\n```\n:::\n\n\nNext we will import the PushshiftAPI from psaw and initialize it.\n\n::: {.cell ExecuteTime='{\"end_time\":\"2022-11-07T15:39:27.774482Z\",\"start_time\":\"2022-11-07T15:39:27.340670Z\"}' execution_count=3}\n``` {.python .cell-code}\nfrom psaw import PushshiftAPI\n\n# Initialize PushShift\napi = PushshiftAPI()\n```\n:::\n\n\n## Collect Reddit Posts (By Subreddit)\n\nTo collect Reddit posts, we will use `api.search_submissions()` and then establish the parameters of our query, such as which subreddit we want to search in and what threshold of upvote score we want to set.\n\nBelow we are setting up to search for posts in the subreddit \"AmITheAsshole\" that have an upvote score of at least 2,000 or more.\n\n::: {.cell ExecuteTime='{\"end_time\":\"2022-11-07T15:39:30.702995Z\",\"start_time\":\"2022-11-07T15:39:30.697210Z\"}' execution_count=4}\n``` {.python .cell-code}\napi_request_generator = api.search_submissions(subreddit='AmITheAsshole',\n                                               score = \">2000\")\n```\n:::\n\n\nOnce this generator is set up, we can use it to collect Reddit posts. The code below is a list comprehension that loops through the generator and extracts relevant data for each matching Reddit post. It then turns that list into a Pandas DataFrame.\n\n<div class=\"admonition pandasreview\" name=\"html-admonition\" style=\"background: black; color: white; padding: 10px\">\n<p class=\"title\">Pandas</p>\n Do you need a refresher or introduction to the Python data analysis library Pandas? Be sure to check out <a href=\"https://melaniewalsh.github.io/Intro-Cultural-Analytics/Data-Analysis/Pandas-Basics-Part1.html\"> Pandas Basics (1-3) </a> in this textbook!\n    \n</div>\n\n::: {.cell ExecuteTime='{\"end_time\":\"2022-11-07T11:15:32.938863Z\",\"start_time\":\"2022-11-07T11:15:32.914487Z\"}' execution_count=5}\n``` {.python .cell-code}\naita_submissions = pd.DataFrame([submission.d_ for submission in api_request_generator])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/paschalis/opt/anaconda3/lib/python3.8/site-packages/psaw/PushshiftAPI.py:252: UserWarning:\n\nNot all PushShift shards are active. Query results may be incomplete\n\n```\n:::\n:::\n\n\n*The cell above should take a while to run. It's searching through Reddit's entire history. It's ok if you periodically get errors while it's running.*\n\nLet's check to see how many Reddit posts we have collected by checking the shape of the DataFrame.\n\n::: {.cell ExecuteTime='{\"end_time\":\"2022-11-07T11:15:33.876499Z\",\"start_time\":\"2022-11-07T11:15:33.855189Z\"}' execution_count=6}\n``` {.python .cell-code}\naita_submissions.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n(2640, 80)\n```\n:::\n:::\n\n\nWe have 2,959 posts!\n\nLet's check to see what columns/metadata exist in this data by seeing what columns are in the DataFrame.\n\n::: {.cell ExecuteTime='{\"end_time\":\"2022-11-07T11:15:35.461379Z\",\"start_time\":\"2022-11-07T11:15:35.451925Z\"}' execution_count=7}\n``` {.python .cell-code}\naita_submissions.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nIndex(['all_awardings', 'allow_live_comments', 'author',\n       'author_flair_css_class', 'author_flair_richtext', 'author_flair_text',\n       'author_flair_type', 'author_fullname', 'author_is_blocked',\n       'author_patreon_flair', 'author_premium', 'awarders', 'can_mod_post',\n       'contest_mode', 'created_utc', 'domain', 'full_link', 'gilded',\n       'gildings', 'id', 'is_created_from_ads_ui', 'is_crosspostable',\n       'is_meta', 'is_original_content', 'is_reddit_media_domain',\n       'is_robot_indexable', 'is_self', 'is_video',\n       'link_flair_background_color', 'link_flair_richtext',\n       'link_flair_text_color', 'link_flair_type', 'locked', 'media_only',\n       'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n       'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n       'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler',\n       'stickied', 'subreddit', 'subreddit_id', 'subreddit_subscribers',\n       'subreddit_type', 'suggested_sort', 'thumbnail', 'title',\n       'total_awards_received', 'treatment_tags', 'upvote_ratio', 'url',\n       'whitelist_status', 'wls', 'created', 'edited', 'removed_by_category',\n       'post_hint', 'preview', 'link_flair_css_class',\n       'link_flair_template_id', 'link_flair_text',\n       'author_flair_background_color', 'author_flair_text_color', 'banned_by',\n       'author_flair_template_id', 'top_awarded_type', 'author_cakeday',\n       'steward_reports', 'updated_utc', 'og_description', 'og_title',\n       'rte_mode'],\n      dtype='object')\n```\n:::\n:::\n\n\nTo get a quick peak of the data, we can look at 10 random rows of data, and only for the columns \"title\" and upvote \"score.\"\n\n::: {.cell ExecuteTime='{\"end_time\":\"2022-11-07T11:15:39.174801Z\",\"start_time\":\"2022-11-07T11:15:37.378887Z\"}' execution_count=8}\n``` {.python .cell-code}\n# aita_submissions[['title', 'score']].sample(10)\n```\n:::\n\n\nTo sort by and manipulate date information, let's transform the date into datetime values.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# aita_submissions['date'] = pd.to_datetime(aita_submissions['created_utc'], utc=True, unit='s')\n```\n:::\n\n\nTo only look at columns of interest, we can insert them in double square brackets.\n\n::: {.cell tags='[\"output_scroll\"]' execution_count=10}\n``` {.python .cell-code}\n# aita_submissions[['author', 'date', 'title', 'selftext', 'url', 'subreddit', 'score', 'num_comments', 'num_crossposts']]\n```\n:::\n\n\n## Collect Reddit Posts (By Keyword)\n\nTo search by a keyword, we will add `q=` and insert a query phrase, the rapper \"Missy Elliott.\"\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# api_request_generator = api.search_submissions(q='Missy Elliott', score = \">2000\")\n```\n:::\n\n\nOnce this generator is set up, we can use it to collect Reddit posts. The code below is a list comprehension that loops through the generator and extracts relevant data for each matching Reddit post. It then turns that list into a Pandas DataFrame.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# missy_submissions = pd.DataFrame([submission.d_ for submission in api_request_generator])\n```\n:::\n\n\n*The cell above should take a while to run. It's searching through Reddit's entire history. It's ok if you periodically get errors while it's running.*\n\nTo sort by and manipulate date information, let's transform the date into datetime values.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# missy_submissions['date'] = pd.to_datetime(missy_submissions['created_utc'], utc=True, unit='s')\n```\n:::\n\n\nTo only look at columns of interest, we can insert them in double square brackets.\n\n::: {.cell tags='[\"output_scroll\"]' execution_count=14}\n``` {.python .cell-code}\n# missy_submissions[['author', 'date', 'title', 'selftext', 'url', 'subreddit', 'score', 'num_comments', 'num_crossposts', ]]\n```\n:::\n\n\nTo look at the subreddits where \"Missy Elliott\" appears the most often, we can use the `.value_counts()` method.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# missy_submissions['subreddit'].value_counts()\n```\n:::\n\n\n## Collect Reddit Comments (By Keyword)\n\nTo collect Reddit *comments* rather than posts, we can use `api.search_comments()` rather than `api.search_submissions()`.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# api_request_generator = api.search_comments(q='Missy Elliott', score = \">2000\")\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# missy_comments = pd.DataFrame([comment.d_ for comment in api_request_generator])\n```\n:::\n\n\n## Collect Reddit Posts and Comments (By Multiple Keywords)\n\nTo search for multiple phrases in posts —  such as posts that mention the author George Orwell OR the author J.R.R. Tolkein — we can use parentheses and the bitwise OR operator `|`\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# api_request_generator = api.search_comments(q='(George Orwell)|(J. R. R. Tolkien)')\n```\n:::\n\n\nTo search for multiple phrases in posts —  such as posts that mention Shakespeare AND Beyonce — we can use parentheses and the bitwise AND operator `&`\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# api_request_generator = api.search_comments(q='(Shakespeare)&(Beyonce)')\n```\n:::\n\n\n## Collect Reddit Posts and Comments (By Date Range)\n\n::: {.cell ExecuteTime='{\"end_time\":\"2022-11-07T11:16:06.071358Z\",\"start_time\":\"2022-11-07T11:16:06.067913Z\"}' execution_count=20}\n``` {.python .cell-code}\n# import datetime as dt\n\n# start_epoch=int(dt.datetime(2020, 1, 1).timestamp())\n# end_epoch=int(dt.datetime(2020, 1, 10).timestamp())\n\n# api_request_generator = api.search_comments(q='(Shakespeare)&(Beyonce)\"', after = start_epoch, before=end_epoch)\n```\n:::\n\n\n\n",
    "supporting": [
      "22-Reddit-Data_files"
    ],
    "filters": [],
    "includes": {}
  }
}